\section{Motivation}

L’analyse par composantes principales est une technique de réduction de la dimension d’un jeu de données. Elle permet de projeter les données dans un espace restreint tout en conservant le maximum de variance entre les données. 
Les axes utilisés par l’analyse sont appelées composantes principales. Ces composantes principales permettent de transposer les 
données en utilisant les meilleures combinaisons linéaires. Les composantes sont ordonnées de telle sorte qu’elles conservent
 une proportion décroissante de la variance du jeu de données. Les premières composantes sont généralement beaucoup plus 
 informatives que les dernières. Il est ainsi possible de réduire la dimension d’un jeu de données en ne considérant que les premières composantes principales tout en conservant une quantité d’information satisfaisante.\\

Mathématiquement, l’ACP s'exécute en décomposant la matrice de variance-covariance (ou de corrélation) estimée avec le jeu de données en valeurs et vecteurs propres. La matrice de variance-covariance est définie telle que

$$C = Var(X) = \frac{1}{N} \sum_{i = 1}^{N}   \textbf{x}_i\textbf{x}_i^{T} = \frac{1}{N} \sum_{i = 1}^{N}   \textbf{x}_i \cdot \textbf{x}_i$$

La principale limitation de cette méthodologie est qu’elle ne permet pas d’exploiter l’information contenue dans les interactions non linéaires entre les attributs. De plus, vu que les composantes principales sont indépendantes les unes entres elles, l'information non linéaire peut réapparaitre dans les données en conservant un grand nombre de composantes principales. \\

Par exemple, avec les données dans la figure suivante n'ont aucune de relation linéaire. 

\begin{figure}[H]
	\centering
	\includegraphics[width=2in]{sim-cercle}
\end{figure}

Ensuite, on applique deux exemples de transformations non linéaires sur les données et on obtient 

\begin{figure}[H]
	\includegraphics[width=\textwidth]{sim-kernel}
\end{figure}

On remarque que ces données peuvent être séparés et interprétés linéairement. On peut donc appliquer une ACP sur ces transformations et on obtient 

\begin{figure}[H]
	\includegraphics[width=\textwidth]{sim-1pc}
\end{figure}

L'idée de l'espace des attributs est de projeter les données originales par une fonction non linéaire vers un nouvel espace qui permet d'interpréter linéairement les relations entre les attributs. Formellement, on a 

\begin{align}\label{eq:featurespace}
\Phi : \mathbb{R}^N &\to \mathcal{F} \nonumber \\
\textbf{x} &\mapsto \Phi(\textbf{x}) 
\end{align}

où les données $\textbf{x}_1, \textbf{x}_2. \dots, \textbf{x}_N \in \mathbb{R}^p$ est projeté vers un espace d'attributs $\mathcal{F}$ \cite{muller2001introduction}. Souvent, la dimension de $\mathcal{F}$ est beaucoup plus élevée que $p$. L'apprentissage statistique peut maintenant être fait sur les données $$(\Phi(\textbf{x}_1), y_1), (\Phi(\textbf{x}_2), y_2), \dots, (\Phi(\textbf{x}_n), y_n).$$

Afin de motiver cette généralisation du modèle, considérons le jeu de donnée avec $p = 2$, où
$$\textbf{x}_1, \textbf{x}_2, \dots, \textbf{x}_N = (x_{1,1},x_{1,2}), (x_{2,1}, x_{2,2}), …, (x_{N,1}, x_{N,2})$$

et la transformation $\Phi(\textbf{x})= (x_1^2 + \sqrt{2} \times x_1\times x_2 + x_2^2)$. Visuellement, on projète les données de cercles vers une demi-sphère, où les données dans le centre cercle ont une profondeur faible et les données aux périmètre du cercle ont une profondeur élevée. De plus, la matrice de variance covariance à utiliser pour l’ACP devient
$$\overline{C} = \frac{1}{N} \sum_{i = 1}^{N}   \Phi(\textbf{x}_i) \Phi(\textbf{x}_i)^{T}.$$

Ensuite, on peut reformuler le produit scalaire entre les deux espaces d'attributs comme

\begin{align*}
(\Phi(\textbf{x}_i) \cdot \Phi(\textbf{x}_j)) &= (x_{i, 1}^2 + \sqrt{2} x_{i, 1} x_{i, 2} + x_{i, 2}^2)(x_{j, 1}^2 + \sqrt{2} x_{j, 1} x_{j, 2} + x_{j, 2}^2)^T\\
&= ((x_{i, 1}x_{i, 2})(x_{j, 1}x_{j, 2})^T)^2\\
&= (\textbf{x}_i \cdot \textbf{x}_j)^2.
\end{align*}

En remplacant avec le résultat précédent, on obtient
$$\overline{C} = \frac{1}{N} \sum_{i = 1}^{N} (\textbf{x}_i \cdot \textbf{x}_j)^2$$ et on doit appliquer une décomposition par valeurs et vecteurs propres sur cette matrice pour effectuer une ACP avec noyau, où le noyau est $(\textbf{x}_i \cdot \textbf{x}_j)^2$.
