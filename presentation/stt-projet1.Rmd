---
title: "ACP avec noyau"
author: "Jean-Thomas Baillargeon et Christopher Blier-Wong"
date: "4 avril 2018"
bibliography: ref.bib
output: 
  revealjs::revealjs_presentation:
    theme: moon
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(kernlab)
library(plotly)
library(readr)
library(dplyr)
set.seed(1337)
```

## Plan

- Motivation
- ACP avec noyau
- Fonction noyau
- Application pratique 
- Discussion

## Motivation

- L'ACP reduit la dimention tout en conservant le maximum de variance dans les données.
- Premières composante les plus informatives.
- Information non linéaire pas facile à capturer

## Exemple 

```{r, echo = FALSE}
x <- -5:5
y <- c(1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1)

plot(x, rep(1, 11), col = y + 2, pch = 20, ylab = "")

```

## Exemple 

```{r, echo = FALSE}
x <- -5:5
y <- c(1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1)

plot(x, x ^ 2, col = y + 2, pch = 20, ylab = "")
```

## Exemple 

```{r, echo = FALSE}
x <- -5:5
y <- c(1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1)

plot(x, x ^ 2, col = y + 2, pch = 20, ylab = "")
abline(h = 2.5)
```

## Autre exemple 

```{r}
angles <- runif(200, 0, 2 * pi)
radius <- runif(200, c(0, 7), c(2, 9))
circle_class <- rep(c(2,4), 100)
x <- radius * cos(angles)
y <- radius * sin(angles)

circle_data <- cbind(x, y)

plot(circle_data, col = circle_class/2 + 5, asp = 1, pch = 19)

```

## Autre exemple

Soit la transformation 

$$\Phi(\textbf{x}) = (x^2, \sqrt{2}xy, y^2).$$
En appliquant cette transformation sur les données originales, on obtient un problème linéaire.

## Autre exemple

```{r}
phi_data = function(x,y) {
  return(c(x^2,sqrt(2)*x*y,y^2))
}
projected_data =numeric(600)
for(index_to_project in 1:200){
  point_to_project = circle_data[index_to_project,]
  projected_point = phi_data(point_to_project[1],point_to_project[2])
  
  projected_data[(3*(index_to_project-1)+1):(3*(index_to_project))] = projected_point
}
projected_data = matrix(projected_data, nrow=200, ncol=3, byrow = TRUE)
projected_data_as_dataframe = as.data.frame(projected_data)
colnames(projected_data_as_dataframe) = c("x", "y", "z")
plotly::plot_ly(projected_data_as_dataframe, x=~x, y=~y, z=~z, color=circle_class) %>%
  add_markers() %>%
  layout(scene = list(xaxis = list(title = 'x'),
                      yaxis = list(title = 'y'),
                      zaxis = list(title = 'x*y')))
```

## Autre exemple

```{r}
phi_data = function(x,y) {
  return(c(x^2,sqrt(2)*x*y,y^2))
}
projected_data =numeric(600)
for(index_to_project in 1:200){
  point_to_project = circle_data[index_to_project,]
  projected_point = phi_data(point_to_project[1],point_to_project[2])
  
  projected_data[(3*(index_to_project-1)+1):(3*(index_to_project))] = projected_point
}
projected_data = matrix(projected_data, nrow=200, ncol=3, byrow = TRUE)
projected_data_as_dataframe = as.data.frame(projected_data)
colnames(projected_data_as_dataframe) = c("x", "y", "z")
pca <- princomp(projected_data_as_dataframe)
plot(pca$score[,1],pca$scores[,2], col=circle_class/2 + 5,main="Données transformées selon les composantes principales", xlab="Première composante principale", ylab="Deuxième composante principale", pch = 19)

```


## Rappel de l'ACP

Mathématiquement, les composantes principales sont obtenues selon une décomposition par valeurs et vecteurs propres de la matrice de covariances 

$$ \frac{1}{N} \sum_{i = 1}^{N} \textbf{x}_i\textbf{x}_i^T.$$
Correspond à la meilleure projection \textbf{linéaire} des données dans une dimension réduite.


## ACP avec noyau

On applique une ACP sur les données projetées [@scholkopf1997kernel]. On décompose la matrice de covariances de la matrice des transformations non linéaires

$$ \frac{1}{N} \sum_{i = 1}^{N} \Phi(\textbf{x}_i)\Phi(\textbf{x}_i)^T = \frac{1}{N} \sum_{i = 1}^{N} \textrm{k}(\textrm{x}_i, \textrm{x}_i) .$$


## Transformation quadratique

Pour une données à deux attributs $\textrm{x} = (x_1, x_2)$, on applique la transformation $\Phi(\textbf{x}) = (x_1^2, \sqrt{2}x_1x_2, x_2^2)$. Le produit scalaire devient

\[
\begin{aligned}
(\Phi(\textbf{x}_i) \cdot \Phi(\textbf{x}_j)) &= (x_{i, 1}^2 + \sqrt{2} x_{i, 1} x_{i, 2} + x_{i, 2}^2)(x_{j, 1}^2 + \sqrt{2} x_{j, 1} x_{j, 2} + x_{j, 2}^2)^T\\
&= ((x_{i, 1}x_{i, 2})(x_{j, 1}x_{j, 2})^T)^2\\
&= (\textbf{x}_i \cdot \textbf{x}_j)^2.
\end{aligned}
\]

On remarque qu'on n'a pas besoin de calculer les valeurs $x_1^2, \sqrt{2}x_1x_2$ ou $x_2^2$.

## Projection dans un autre espace d'attributs 

-En général, on projète les données $\textbf{x}$ dans un espace d'attributs par une transformation non linéaire.

-Formellement, on a 

\[
\begin{aligned}
\Phi: \mathbb{R}^N &\to \mathcal{F}\\
\textbf{x} &\mapsto \Phi(\textbf{x})
\end{aligned}
\]

- Au lieu de faire l'apprentissage statistique sur $\textbf{x}_1, \textbf{x}_2, \dots, \textbf{x}_N$, on le fait sur $\Phi(\textbf{x}_1), \Phi(\textbf{x}_2), \dots, \Phi(\textbf{x}_N)$. 



## Fonction noyau


- Pour l'ACP avec noyau, on a seulement besoin du produit scalaire
- $\Phi$ peut être une fonction non définie ou de dimension infinie, tant que son produit scalaire existe.
- Soit $$k(\textrm{x}, \textrm{y}) = \Phi(\textrm{x})\cdot \Phi(\textrm{y}).$$
Alors, on peut écrire la matrice de covariance de $\Phi$ comme

$$\frac{1}{N} \sum_{i = 1}^{N} \Phi(\textbf{x}_i) \Phi(\textbf{x}_i)^T = \frac{1}{N} \sum_{i = 1}^{N} \textrm{k}(\textrm{x}_i, \textrm{x}_i).$$



## Transformation polynomiale

Pour le polynome multinomial, on a 

$$\Phi(\textrm{x}) = \sum_{\textbf{J} \in \{0, 1, \dots, N\}} w_{\textrm{J}}  \prod_{i = 1}^{k}x_{J_i}.$$
Ce vecteur contient les données toutes les combinaisons polynomiales de dimension $k$. Par exemple, 

$$x_1^k, x_2^k, \dots, x_p^k,$$
$$x_1, x_1^2,\dots, x_1^{k-1}, x_1^{k},$$
$$x_1^{k-1}x_2, x_1^{k-2}x_2^2, \dots, x_1 x_2^{k-1},$$
$$x_1^{k-1}x_2, x_1^{k-1}x_3, \dots, x_1^{k-1}x_p$$

## Transformation polynomiale

La transformation produit des données dans une dimension$(p + 1)^k$. Par contre, on peut montrer que 

$$\Phi(\textrm{x})\cdot \Phi(\textrm{y}) = (1 + \textrm{x}\cdot \textrm{y})^k = \textrm{k}(\textrm{x}, \textrm{y}).$$

## Transformation gaussienne

Soit la transformation 

$$\Phi_k(\textrm{x}) = \frac{1}{\sqrt{k!}}e^{-\frac{||\textrm{x}||^2}{2}}\prod_{i = 1}^N\frac{x_{J_i}}{\sigma}    .$$

On considère l'espace d'attributs de toutes les combinaisons de $\Phi_k(\textrm{x}), k\in \mathbb{N}$. 

Alors, on a 

$$\Phi(\textrm{x})\cdot \Phi(\textrm{y}) = e^{-\frac{||\textrm{x} - \textrm{y}||^2}{2\sigma^2}} = \textrm{k}(\textrm{x}, \textrm{y}).$$

## Normalisation des données

- La formule $$\frac{1}{N} \sum_{i = 1}^{N} \textrm{k}(\textrm{x}, \textrm{x})$$ est valide si les données sont centrées, c-à-d que $$\sum_{i = 1}^{N} \Phi(\textrm{x}_i) = 0.$$

- Si $\Phi(\textrm{x})$ est de dimension infinie, il est impossible de vérifier cette hypothèse. 

## Normalisation des données 

La solution est proposée par [@scholkopf1997kernel]

Soit $$K_{ij} = (k(\textbf{x}_i, \textbf{x}_j))_{ij} \textrm{ et } K,$$

la matrice des $K_{ij}$. Alors, on peut centrer la matrice $\tilde{K}$ selon

$$\tilde{K}_{ij} = (K - 1_NK - K1_N + 1_NK1_N)_{ij}$$

où $(1_N)_{ij} := \frac{1}{N}$ et éviter de calculer les données $\Phi(\textbf{x})$. 

## Lien avec le SVM
Rappel : On veut maximiser $M$ tel que 
$$y_i(\beta_0 + \beta_1 x_{i1} \dots + \beta_p x_{ip}) \geq M(1 - \varepsilon_i) ~ \forall ~ i = 1, \dots, n$$
sous les contraintes 
\[
\begin{aligned}
\sum_{j= 0}^p\beta_j^2 &= 1\\
\varepsilon_i &\geq 0 ~\forall~i\\
\sum_{i = 1}^n\varepsilon_i &\leq c
\end{aligned}
\]
 	 
## Classifieur à vecteurs de support : formulation duale avec marges douces
 	 
Avec les multiplicateurs de Lagrange, la fonction à maximiser devient

$$-\frac{1}{2} \sum_{t = 1}^n\sum_{s = 1}^n \alpha_t\alpha_s r_tr_s\textbf{x}_t\textbf{x}_s^T + \sum_{t = 1}^n \alpha_t$$
sous les contraintes 

\[
 	 \begin{aligned}
 	 \sum_{t= 1}^n\alpha_tr_t &= 0\\
 	 0\leq \alpha_i &\leq c ~\forall~i\\
 	 \end{aligned}
\]

## Applications MNIST
 	 
On applique l'ACP avec noyau sur le jeu de données MNIST [@lecun1998gradient].
```{r, echo=FALSE, show = FALSE, fig.show=TRUE, show = FALSE, message = FALSE}
library(readr)

set.seed(25032018)

mnist_raw <- read_csv("https://pjreddie.com/media/files/mnist_train.csv", col_names = FALSE)

instances_to_use <- 2000

mnist_data <- mnist_raw[1:instances_to_use, -1] %>% as.matrix
mnist_label <- mnist_raw[1:instances_to_use, 1] %>% unlist %>% as.vector + 1
par(mfrow = c(1, 4))
for(i in 1:4) {
  mat <- matrix(mnist_data[i, ], 28, 28, byrow = TRUE)
  mat <- apply(mat, 1, rev)
  mat <- apply(mat, 2, rev)
  mat <- apply(mat, 1, rev)
  image(t(mat), col = gray((0:255)/255))
}
```


##  Applications MNIST

```{r}
kpc_rbf1 <- kpca(mnist_data, kernel = "rbfdot", kpar = list(sigma = 0.0000004), features = 2)
kpc_rbf2 <- kpca(mnist_data, kernel = "rbfdot", kpar = list(sigma = 0.000005), features = 2)
kpc_bessel <- kpca(mnist_data, kernel = "besseldot", kpar = list(sigma = 0.01, order = 10), features = 2)
kpc_tanh <- kpca(mnist_data, kernel = "tanhdot", kpar = list(scale = 0.000005), features = 2)
kpc_poly <- kpca(mnist_data, kernel = "polydot", kpar = list(degree = 3), features = 2)

par(mfrow = c(2, 3))

pc <- prcomp(mnist_data, rank. = 2)

plot(pc$x %*% matrix(c(-1, 0, 0, -1), 2), col = mnist_label, pch = 19, main = "pca", xlab = "CP1", ylab = "CP2")
plot(rotated(kpc_rbf1), col = mnist_label, pch = 19, main = "kpca rbf 1", xlab = "CP1", ylab = "CP2")
plot(rotated(kpc_rbf2), col = mnist_label, pch = 19, main = "kpca rbf 2", xlab = "CP1", ylab = "CP2")
plot(rotated(kpc_bessel), col = mnist_label, pch = 19, main = "kpca Bessel", xlab = "CP1", ylab = "CP2")
plot(rotated(kpc_tanh), col = mnist_label, pch = 19, main = "kpca tanh", xlab = "CP1", ylab = "CP2")
plot(rotated(kpc_poly), col = mnist_label, pch = 19, main = "kpca polynomial", xlab = "CP1", ylab = "CP2")
```

## Discussion

### Avantages
- Découvrir les relations non linéaires entre les données
- Possiblement réduire davantage la dimension 
- Noyau = truc utile lorsque seulement le produit scalaire est requis
- Succès dans détection de visages [@kim2002face]

## Discussion

### Incovénients
- Ajuster les hyper-paramètres
- Non-supervisé : l'ajustement des hyper-paramètres peut être compliqué
- Ne permet pas de reconstruire les données

## Références